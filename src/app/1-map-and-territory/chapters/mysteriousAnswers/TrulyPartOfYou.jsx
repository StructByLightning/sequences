import Chapter from "@/components/Chapter/Chapter";

export default function TrulyPartOfYou() {


  return <Chapter title={["Truly Part of You"]}>
    <p>A classic paper by Drew McDermott, “Artificial Intelligence Meets Natural Stupidity,” criticized AI programs that would try to represent notions like <em>happiness is a state of mind</em> using a semantic network: <span className="footnote">Drew McDermott, “Artificial Intelligence Meets Natural Stupidity,” <em>SIGART Newsletter</em>, no. 57 (1976): 4–9, doi:10.1145/1045339.1045340.</span></p>
    <p><code>STATE-OF-MIND &lt;--IS-A--- HAPPINESS</code></p>
    <p>And of course there’s nothing <em>inside</em> the <code>HAPPINESS</code> node; it’s just a naked lisp token with a suggestive English name.</p>
    <p>So, McDermott says, “A good test for the disciplined programmer is to try using gensyms in key places and see if he still admires his system. For example, if <code>STATE-OF-MIND</code> is renamed G1073 . . .” then we would have <code>IS-A(HAPPINESS,G1073)</code> “which looks much more dubious.”</p>
    <p>Or as I would slightly rephrase the idea: If you substituted randomized symbols for <em>all</em> the suggestive English names, you would be completely unable to figure out what G1071(G1072,G1073) meant. Was the AI program meant to represent hamburgers? Apples? Happiness? Who knows? <em>If you delete the suggestive English names, they don’t grow back.</em></p>
    <p>Suppose a physicist tells you that “Light is waves,” and you <em>believe</em> the physicist. You now have a little network in your head that says:</p>
    <p><code>IS-A(LIGHT, WAVES)</code></p>
    <p>If someone asks you “What is light made of?” you’ll be able to say “Waves!” As McDermott says, “The whole problem is getting the hearer to notice what it has been told. Not ‘understand,’ but ‘notice.’ ” Suppose that instead the physicist told you, “Light is made of little curvy things.” (Not true, by the way.) Would you <em>notice</em> any difference of anticipated experience?</p>
    <p>How can you realize that you shouldn’t trust your seeming knowledge that “light is waves”? One test you could apply is asking, “Could I <em>regenerate</em> this knowledge if it were somehow deleted from my mind?”</p>
    <p>This is similar in spirit to scrambling the names of suggestively named lisp tokens in your AI program, and seeing if someone else can figure out what they allegedly “refer” to. It’s also similar in spirit to observing that an Artificial Arithmetician programmed to record and play back</p>
    <p><code>Plus-Of(Seven, Six) = Thirteen</code></p>
    <p>can’t regenerate the knowledge if you delete it from memory, until another human re-enters it in the database. Just as if you forgot that “light is waves,” you couldn’t get back the knowledge except the same way you got the knowledge to begin with—by asking a physicist. You couldn’t generate the knowledge for yourself, the way that physicists originally generated it.</p>
    <p>The same experiences that lead us to formulate a belief, connect that belief to other knowledge and sensory input and motor output. If you see a beaver chewing a log, then you know what this thing-that-chews-through-logs looks like, and you will be able to recognize it on future occasions whether it is called a “beaver” or not. But if you acquire your beliefs about beavers by someone else telling you facts about “beavers,” you may not be able to recognize a beaver when you see one.</p>
    <p>This is the terrible danger of trying to <em>tell</em> an Artificial Intelligence facts that it could not learn for itself. It is also the terrible danger of trying to <em>tell</em> someone about physics that they cannot verify for themselves. For what physicists mean by “wave” is not “little squiggly thing” but a purely mathematical concept.</p>
    <p>As Davidson observes, if you believe that “beavers” live in deserts, are pure white in color, and weigh 300 pounds when adult, then you do not have any beliefs <em>about</em> beavers, true or false. Your belief about “beavers” is not right enough to be wrong.<span className="footnote">Richard Rorty, “Out of the Matrix: How the Late Philosopher Donald Davidson Showed That Reality Can’t Be an Illusion,” <em>The Boston Globe</em> (October 2003).</span> If you don’t have enough experience to regenerate beliefs when they are deleted, then do you have enough experience to connect that belief to anything at all? Wittgenstein: “A wheel that can be turned though nothing else moves with it, is not part of the mechanism.”</p>
    <p>Almost as soon as I started reading about AI—even before I read McDermott—I realized it would be a <em>really good idea</em> to always ask myself: “How would I regenerate this knowledge if it were deleted from my mind?”</p>
    <p>The deeper the deletion, the stricter the test. If all proofs of the Pythagorean Theorem were deleted from my mind, could I re-prove it? I think so. If all knowledge of the Pythagorean Theorem were deleted from my mind, would I notice the Pythagorean Theorem to re-prove? That’s harder to boast, without putting it to the test; but if you handed me a right triangle with sides of length 3 and 4, and told me that the length of the hypotenuse was calculable, I think I would be able to calculate it, if I still knew all the rest of my math.</p>
    <p>What about the notion of <em>mathematical proof</em> ? If no one had ever told it to me, would I be able to reinvent <em>that</em> on the basis of other beliefs I possess? There was a time when humanity did not have such a concept. Someone must have invented it. What was it that they noticed? Would I notice if I saw something equally novel and equally important? Would I be able to think that far outside the box?</p>
    <p>How much of your knowledge could you regenerate? From how deep a deletion? It’s not just a test to cast out insufficiently connected beliefs. It’s a way of absorbing <em>a fountain of knowledge, not just one fact</em>.</p>
    <p>A shepherd builds a counting system that works by throwing a pebble into a bucket whenever a sheep leaves the fold, and taking a pebble out whenever a sheep returns. If you, the apprentice, do not understand this system—if it is magic that works for no apparent reason—then you will not know what to do if you accidentally drop an extra pebble into the bucket. That which you cannot make yourself, you cannot <em>remake</em> when the situation calls for it. You cannot go back to the source, tweak one of the parameter settings, and regenerate the output, without the source. If “two plus four equals six” is a brute fact unto you, and then one of the elements changes to “five,” how are you to know that “two plus five equals seven” when you were simply <em>told</em> that “two plus four equals six”?</p>
    <p>If you see a small plant that drops a seed whenever a bird passes it, it will not occur to you that you can use this plant to partially automate the sheep-counter. Though you learned something that the original maker would use to improve on their invention, you can’t go back to the source and re-create it.</p>
    <p>When you contain the source of a thought, that thought can change along with you as you acquire new knowledge and new skills. When you contain the source of a thought, it becomes truly a part of you and grows along with you.</p>
    <p>Strive to make yourself the source of every thought worth thinking. If the thought originally came from outside, make sure it comes from inside as well. Continually ask yourself: “How would I regenerate the thought if it were deleted?” When you have an answer, imagine <em>that</em> knowledge being deleted as well. And when you find a fountain, see what else it can pour.</p>
  </Chapter>
}